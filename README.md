
# ğŸ­ Facial Emotion Recognition

<div align="center">

![Python](https://img.shields.io/badge/Python-3.10+-3776AB?style=for-the-badge&logo=python&logoColor=white)
![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-EE4C2C?style=for-the-badge&logo=pytorch&logoColor=white)
![OpenCV](https://img.shields.io/badge/OpenCV-4.8+-5C3EE8?style=for-the-badge&logo=opencv&logoColor=white)
![License](https://img.shields.io/badge/License-MIT-22C55E?style=for-the-badge)
![Status](https://img.shields.io/badge/Status-Active-22C55E?style=for-the-badge)

**Real-time facial emotion recognition from webcam using a fine-tuned ResNet-18.**
7 universal emotion classes Â· Temporal smoothing Â· Grad-CAM visualization Â· Class-balanced training.

[Overview](#-overview) Â· [Architecture](#-architecture) Â· [Setup](#-setup) Â· [Training](#-training) Â· [Inference](#-real-time-inference) Â· [Results](#-results) Â· [References](#-references)

</div>

---

## ğŸ“– Overview

This project implements a complete end-to-end pipeline for **real-time Facial Emotion Recognition (FER)** â€” from raw dataset to live webcam inference. Every component is built and owned: data loading and augmentation, transfer learning fine-tuning, loss function design for class imbalance, and a live inference loop with temporal smoothing.

**What this is not:** a wrapper around a cloud API. Every design decision â€” which layers to freeze, how to handle the `disgust` class having 16Ã— fewer samples than `happy`, why temporal smoothing over 10 frames matters â€” is implemented and justified from first principles, grounded in the academic literature.

### Recognized Emotions

| Label | Description |
|-------|-------------|
| ğŸ˜  `angry` | Raised inner brows, lip corners pulled down |
| ğŸ¤¢ `disgust` | Nose wrinkle, upper lip raise |
| ğŸ˜¨ `fear` | Wide eyes, raised upper lip |
| ğŸ˜Š `happy` | Lip corner pull, cheek raise |
| ğŸ˜ `neutral` | No dominant muscle activation |
| ğŸ˜¢ `sad` | Inner brow raise, lip corner depression |
| ğŸ˜² `surprise` | Wide eyes, dropped jaw |

---

## ğŸ— Architecture

The pipeline follows a modular, sequential design:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        WEBCAM FRAME (BGR)                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â”‚
                               â–¼
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚      HAAR CASCADE DETECTOR      â”‚
              â”‚   (Viola-Jones face localizer)  â”‚
              â”‚   Output: (x, y, w, h) boxes    â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â”‚  crop per face
                               â–¼
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚         PREPROCESSING           â”‚
              â”‚  Grayscale â†’ Resize 48Ã—48       â”‚
              â”‚  Normalize Î¼=0.507, Ïƒ=0.255     â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â”‚
                               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      ResNet-18 BACKBONE                           â”‚
â”‚                                                                    â”‚
â”‚   Conv1(1â†’64, 3Ã—3, s=1) â”€â”€â–º BN â”€â”€â–º ReLU                         â”‚
â”‚         â”‚                                                          â”‚
â”‚       Layer1 (2Ã— BasicBlock, 64ch)                                â”‚
â”‚         â”‚                                                          â”‚
â”‚       Layer2 (2Ã— BasicBlock, 128ch)                               â”‚
â”‚         â”‚                                                          â”‚
â”‚       Layer3 (2Ã— BasicBlock, 256ch)                               â”‚
â”‚         â”‚                                                          â”‚
â”‚       Layer4 (2Ã— BasicBlock, 512ch) â—„â”€â”€ Grad-CAM hook            â”‚
â”‚         â”‚                                                          â”‚
â”‚   AdaptiveAvgPool(1Ã—1) â†’ Flatten â†’ 512-dim vector                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â”‚
                               â–¼
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚       CLASSIFICATION HEAD       â”‚
              â”‚   Dropout(0.5)                  â”‚
              â”‚   Linear(512 â†’ 256)             â”‚
              â”‚   BatchNorm1d â†’ ReLU            â”‚
              â”‚   Dropout(0.25)                 â”‚
              â”‚   Linear(256 â†’ 7)               â”‚
              â”‚   Softmax                       â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â”‚
                               â–¼
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚      TEMPORAL SMOOTHER          â”‚
              â”‚   Sliding window N=10 frames    â”‚
              â”‚   Mean of softmax probability   â”‚
              â”‚   vectors over window           â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â”‚
                               â–¼
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚  OVERLAY: label Â· confidence Â· bars    â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Key Design Choices

**Grayscale single-channel input.** FER2013 is grayscale by nature. Processing in grayscale halves memory, speeds training, and avoids the model learning spurious color correlations. The first `Conv1` layer is adapted to accept 1-channel input with Kaiming initialization.

**MaxPool removed.** The standard ResNet-18 uses a 7Ã—7 conv with stride 2 followed by a 3Ã—3 maxpool, which aggressively downsamples early feature maps. For a 48Ã—48 input this reduces spatial resolution too aggressively. `maxpool` is replaced with `nn.Identity()`.

**Staged unfreezing.** Backbone frozen for the first 5 epochs while only the head trains. Then the full network unfreezes with CosineAnnealingLR. This prevents the large early gradients from the randomly-initialized head from destroying pretrained ImageNet features.

**Label smoothing + class weights.** FER2013 is severely imbalanced (`disgust`: 436 samples vs `happy`: 7,215). Hard one-hot labels combined with this imbalance cause the model to ignore minority classes. Label smoothing (Îµ=0.1) distributes probability mass across non-target classes, and per-class weights inversely proportional to class frequency are folded into the loss.

---

## ğŸ“ Project Structure

```
fer_project/
â”‚
â”œâ”€â”€ configs/
â”‚   â””â”€â”€ config.yaml              # Single source of truth for all hyperparameters
â”‚
â”œâ”€â”€ data/
â”‚   â””â”€â”€ fer2013/                 # Dataset root (not tracked by git)
â”‚       â”œâ”€â”€ train/
â”‚       â”‚   â”œâ”€â”€ angry/           # ~3,995 images
â”‚       â”‚   â”œâ”€â”€ disgust/         # ~436 images  â† heavily underrepresented
â”‚       â”‚   â”œâ”€â”€ fear/            # ~4,097 images
â”‚       â”‚   â”œâ”€â”€ happy/           # ~7,215 images â† dominant class
â”‚       â”‚   â”œâ”€â”€ neutral/         # ~4,965 images
â”‚       â”‚   â”œâ”€â”€ sad/             # ~4,830 images
â”‚       â”‚   â””â”€â”€ surprise/        # ~3,171 images
â”‚       â””â”€â”€ test/
â”‚           â””â”€â”€ (same structure)
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â”œâ”€â”€ dataset.py           # FER2013Dataset + automatic class weight computation
â”‚   â”‚   â””â”€â”€ transforms.py        # Train / val / inference transform pipelines
â”‚   â”‚
â”‚   â”œâ”€â”€ model/
â”‚   â”‚   â”œâ”€â”€ resnet_fer.py        # ResNet-18 adapted for 1-ch, 48Ã—48 FER
â”‚   â”‚   â””â”€â”€ cam.py               # Grad-CAM extractor + heatmap overlay
â”‚   â”‚
â”‚   â”œâ”€â”€ training/
â”‚   â”‚   â”œâ”€â”€ trainer.py           # Training loop, TensorBoard, early stopping
â”‚   â”‚   â””â”€â”€ losses.py            # WeightedCrossEntropy + LabelSmoothingLoss
â”‚   â”‚
â”‚   â”œâ”€â”€ inference/
â”‚   â”‚   â”œâ”€â”€ predictor.py         # Single-image predictor + sliding-window smoother
â”‚   â”‚   â””â”€â”€ webcam.py            # Live webcam loop with Haar Cascade + overlay
â”‚   â”‚
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ metrics.py           # AverageMeter, accuracy(), evaluate_model()
â”‚       â””â”€â”€ visualization.py     # Confusion matrix plot, per-class bar overlay
â”‚
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ train.py                 # Training entry point
â”‚   â”œâ”€â”€ evaluate.py              # Test-set evaluation + confusion matrix export
â”‚   â””â”€â”€ run_webcam.py            # Live inference entry point
â”‚
â”œâ”€â”€ checkpoints/                 # Saved .pth files (not tracked)
â”œâ”€â”€ logs/                        # TensorBoard runs + exported figures
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## âš™ï¸ Setup

### 1 â€” Environment

```bash
git clone https://github.com/para0107/Live-Facial-Emotion-Recognition
cd fer_project

python -m venv .venv
source .venv/bin/activate        # Windows: .venv\Scripts\activate

pip install --upgrade pip
pip install -r requirements.txt
```

### 2 â€” GPU check (recommended)

```python
import torch
print("CUDA available:", torch.cuda.is_available())
print("Device:", torch.cuda.get_device_name(0) if torch.cuda.is_available() else "CPU")
```

Training on CPU is possible but will take significantly longer (~8â€“12 hours vs ~45 minutes on a mid-range GPU).

---

## ğŸ“¦ Dataset

Download **FER2013** from Kaggle:
ğŸ”— [https://www.kaggle.com/datasets/msambare/fer2013](https://www.kaggle.com/datasets/msambare/fer2013)

Place the extracted `train/` and `test/` folders inside `data/fer2013/`.

### Dataset statistics

| Class     | Train  | Test  | % of Train | Auto Weight |
|-----------|--------|-------|------------|-------------|
| angry     | 3,995  | 958   | 13.9%      | 1.74        |
| disgust   | 436    | 111   | 1.5%       | **15.94**   |
| fear      | 4,097  | 1,024 | 14.3%      | 1.70        |
| happy     | 7,215  | 1,774 | 25.1%      | 0.97        |
| neutral   | 4,965  | 1,233 | 17.3%      | 1.40        |
| sad       | 4,830  | 1,247 | 16.8%      | 1.44        |
| surprise  | 3,171  | 831   | 11.0%      | 2.19        |
| **Total** | **28,709** | **7,178** | â€” | â€” |

Class weights are computed automatically: `w_c = N_total / (C Ã— N_c)`

---

## ğŸ‹ï¸ Training

```bash
python scripts/train.py
```

All hyperparameters live in `configs/config.yaml`.

### Hyperparameter reference

| Parameter | Value | Rationale |
|-----------|-------|-----------|
| Backbone | ResNet-18 | Good capacity/size ratio for 48Ã—48 images |
| Pretrained | ImageNet | Exploits general visual feature hierarchy |
| Input channels | 1 | FER2013 is grayscale |
| Input size | 48 Ã— 48 | Native FER2013 resolution |
| Freeze epochs | 5 | Head stabilizes before backbone unfreezes |
| Epochs | 50 | With early stopping patience=10 |
| Batch size | 64 | Stable gradients, fits ~4 GB VRAM |
| Learning rate | 2e-4 | Conservative; preserves pretrained weights |
| Weight decay | 1e-4 | L2 regularization |
| Optimizer | Adam | Adaptive LR, robust to sparse gradients |
| Scheduler | CosineAnnealingLR | Smooth decay to eta_min=1e-6 |
| Dropout | 0.5 / 0.25 | Two-stage in classifier head |
| Label smoothing | 0.1 | Prevents overconfident predictions |
| Random erasing | p=0.3 | Forces holistic facial feature use |
| Early stopping | patience=10 | |

### Monitor with TensorBoard

```bash
tensorboard --logdir logs/
# Open http://localhost:6006
```

### Staged training

```
Epochs 1â€“5:   Backbone FROZEN  â†’  head-only training
              Large gradients from random init stay contained

Epoch 6+:     Full network UNFROZEN
              Fine-tunes with low LR decaying via cosine schedule
```

---

## ğŸ“Š Evaluation

```bash
python scripts/evaluate.py --checkpoint checkpoints/best_model.pth
```

Outputs per-class precision, recall, F1 and saves `logs/confusion_matrix.png`.

**Most common confusions on FER2013:**
- `fear` â†” `sad` (both involve downturned features)
- `disgust` â†” `angry` (both involve brow lowering)
- `surprise` â†” `fear` (both involve widened eyes)

These reflect genuine perceptual ambiguity â€” human accuracy on FER2013 is estimated at ~65%.

---

## ğŸ¥ Real-Time Inference

```bash
python scripts/run_webcam.py --checkpoint checkpoints/best_model.pth
```

### Controls

| Key | Action |
|-----|--------|
| `Q` | Quit |
| `R` | Reset temporal smoothing buffer |

### Display elements

- Bounding box colored by dominant emotion
- Label + confidence above the box
- 7-class probability bar chart in frame corner
- Face count bottom-left

### Temporal smoothing

Raw per-frame softmax vectors are averaged over a sliding window:

```
pÌ„_t = (1/N) Î£_{i=0}^{N-1} p_{t-i}     N = 10
```

Eliminates jitter from micro-expressions and brief detection instabilities without learnable parameters. At 30 fps this introduces ~333 ms of latency.

---

## ğŸ”¬ Grad-CAM Visualization

```python
from src.model.cam import CAMExtractor
from src.model.resnet_fer import ResNetFER

model = ResNetFER(pretrained=False)
# load checkpoint ...

extractor = CAMExtractor(model)
cam, predicted_class = extractor.generate_cam(input_tensor, target_class=3)  # 3=happy
overlay = extractor.overlay_cam(original_image, cam)
```

Expected: model attends to mouth/cheeks for `happy`, brow region for `angry`, eye region for `fear`/`surprise`. Diffuse or non-facial attention maps indicate the model is learning dataset artifacts.

---

## ğŸ“ˆ Results

| Configuration | Test Accuracy | Notes |
|---------------|---------------|-------|
| Random baseline | 14.3% | Uniform over 7 classes |
| ResNet-18, no pretrain, scratch | ~52% | Overfits quickly |
| ResNet-18, pretrained, full fine-tune | ~65% | Near human-level |
| + Label smoothing Îµ=0.1 | ~67% | Better calibration |
| + Class-weighted loss | ~67-68% | Disgust F1 improves significantly |
| + Random erasing p=0.3 | ~69% | Holistic features |
| + Staged freeze/unfreeze | **~70-71%** | **Final configuration** |

*Human accuracy on FER2013 â‰ˆ 65%. Results vary across random seeds.*

---

## ğŸ“š References

1. **Zhang et al. (2024).** *Open-Set Facial Expression Recognition.* AAAI 2024. `arXiv:2401.12507`
2. **Schroff et al. (2015).** *FaceNet: A Unified Embedding for Face Recognition and Clustering.* CVPR 2015. `arXiv:1503.03832`
3. **Dewi et al. (2024).** *Real-Time Facial Expression Recognition: Advances, Challenges, and Future Directions.* Vietnam Journal of Computer Science.
4. **He et al. (2016).** *Deep Residual Learning for Image Recognition.* CVPR 2016.
5. **Goodfellow et al. (2013).** *Challenges in Representation Learning* (FER2013 dataset).
6. **Selvaraju et al. (2017).** *Grad-CAM: Visual Explanations from Deep Networks.* ICCV 2017.
7. **Viola & Jones (2001).** *Rapid Object Detection using a Boosted Cascade of Simple Features.* CVPR 2001.

---

<div align="center">
Built as a Bachelor's thesis project in Computer Science.<br/>
Grounded in peer-reviewed FER literature Â· Every design decision justified.
</div>
