# ğŸ­ Facial Emotion Recognition

<div align="center">

![Python](https://img.shields.io/badge/Python-3.10+-3776AB?style=for-the-badge&logo=python&logoColor=white)
![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-EE4C2C?style=for-the-badge&logo=pytorch&logoColor=white)
![OpenCV](https://img.shields.io/badge/OpenCV-4.8+-5C3EE8?style=for-the-badge&logo=opencv&logoColor=white)
![FastAPI](https://img.shields.io/badge/FastAPI-0.100+-009688?style=for-the-badge&logo=fastapi&logoColor=white)
![React](https://img.shields.io/badge/React-18.2+-61DAFB?style=for-the-badge&logo=react&logoColor=black)
![License](https://img.shields.io/badge/License-MIT-22C55E?style=for-the-badge)
![Status](https://img.shields.io/badge/Status-Active-22C55E?style=for-the-badge)

**Real-time facial emotion recognition from webcam using a fine-tuned ResNet-18.**  
7 universal emotion classes Â· Temporal smoothing Â· Uncertainty visualization Â· WebSocket inference Â· Deployable web app.

[Overview](#-overview) Â· [Architecture](#-architecture) Â· [Setup](#-setup) Â· [Training](#-training) Â· [Inference](#-real-time-inference) Â· [Web App](#-web-app) Â· [Results](#-results) Â· [References](#-references)

</div>

---

## ğŸ“– Overview

This project implements a complete end-to-end pipeline for **real-time Facial Emotion Recognition (FER)** , from raw dataset to live webcam inference and a deployable web application. Every component is built from scratch: data loading and augmentation, transfer learning fine-tuning, loss function design for class imbalance, a live inference loop with temporal smoothing, and a React + FastAPI web app with WebSocket streaming.

**What this is not:** a wrapper around a cloud API. Every design decision, which layers to freeze, how to handle the `disgust` class having 16Ã— fewer samples than `happy`, why temporal smoothing over 10 frames matters, when to show uncertainty labels, is implemented and justified from first principles, grounded in the academic literature.

### Recognized Emotions

| Label | Description |
|-------|-------------|
| ğŸ˜  `angry` | Raised inner brows, lip corners pulled down |
| ğŸ¤¢ `disgust` | Nose wrinkle, upper lip raise |
| ğŸ˜¨ `fear` | Wide eyes, raised upper lip |
| ğŸ˜Š `happy` | Lip corner pull, cheek raise |
| ğŸ˜ `neutral` | No dominant muscle activation |
| ğŸ˜¢ `sad` | Inner brow raise, lip corner depression |
| ğŸ˜² `surprise` | Wide eyes, dropped jaw |

---

## ğŸ— Architecture

The pipeline follows a modular, sequential design:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        WEBCAM FRAME (BGR)                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â”‚
                               â–¼
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚      HAAR CASCADE DETECTOR      â”‚
              â”‚   (Viola-Jones face localizer)  â”‚
              â”‚   Output: (x, y, w, h) boxes    â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â”‚  crop per face
                               â–¼
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚         PREPROCESSING           â”‚
              â”‚  Grayscale â†’ Resize 48Ã—48       â”‚
              â”‚  Normalize Î¼=0.507, Ïƒ=0.255     â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â”‚
                               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      ResNet-18 BACKBONE                           â”‚
â”‚                                                                    â”‚
â”‚   Conv1(1â†’64, 7Ã—7, s=2) â”€â”€â–º BN â”€â”€â–º ReLU                         â”‚
â”‚         â”‚                                                          â”‚
â”‚   MaxPool2d(kernel=2, stride=1, padding=0)                       â”‚
â”‚         â”‚                                                          â”‚
â”‚       Layer1 (2Ã— BasicBlock, 64ch)                                â”‚
â”‚         â”‚                                                          â”‚
â”‚       Layer2 (2Ã— BasicBlock, 128ch)                               â”‚
â”‚         â”‚                                                          â”‚
â”‚       Layer3 (2Ã— BasicBlock, 256ch)                               â”‚
â”‚         â”‚                                                          â”‚
â”‚       Layer4 (2Ã— BasicBlock, 512ch) â—„â”€â”€ Grad-CAM hook            â”‚
â”‚         â”‚                                                          â”‚
â”‚   AdaptiveAvgPool(1Ã—1) â†’ Flatten â†’ 512-dim vector                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â”‚
                               â–¼
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚       CLASSIFICATION HEAD       â”‚
              â”‚   Dropout(0.5)                  â”‚
              â”‚   Linear(512 â†’ 256)             â”‚
              â”‚   BatchNorm1d â†’ ReLU            â”‚
              â”‚   Dropout(0.25)                 â”‚
              â”‚   Linear(256 â†’ 7)               â”‚
              â”‚   [raw logits ]     â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â”‚
                               â–¼
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚      TEMPORAL SMOOTHER          â”‚
              â”‚   Sliding window N=10 frames    â”‚
              â”‚   Mean of softmax probability   â”‚
              â”‚   vectors over window           â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â”‚
                               â–¼
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚  UNCERTAINTY CHECK  (threshold = 0.40)         â”‚
          â”‚  top_conf < 0.40 â†’ show top-2 emotions         â”‚
          â”‚  e.g.  "NEUTRAL / ANGRY  34% / 28%"            â”‚
          â”‚  Bounding box rendered in muted yellow         â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â”‚
                               â–¼
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚  OVERLAY: label Â· confidence Â· bars    â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Key Design Choices

**Grayscale single-channel input.** FER2013 is grayscale by nature. Processing in grayscale halves memory, speeds training, and avoids the model learning spurious color correlations. The first `Conv1` layer is adapted to accept 1-channel input by averaging the pretrained ImageNet weights across the 3 input channels (`new_conv.weight = old_conv.weight.mean(dim=1, keepdim=True)`), which preserves the magnitude of learned features.

**Modified MaxPool.** The standard ResNet-18 uses a 3Ã—3 maxpool with stride 2 after `conv1`, which aggressively downsamples early feature maps. For a 48Ã—48 input this reduces spatial resolution too aggressively. `maxpool` is replaced with `MaxPool2d(kernel_size=2, stride=1, padding=0)` , a softer spatial reduction that retains more low-level detail. **Important:** the saved checkpoint `best_model.pth` was trained with this exact configuration. Do not swap it for `nn.Identity()` without retraining.

**Staged unfreezing.** Backbone frozen for the first 5 epochs while only the head trains. Then the full network unfreezes with CosineAnnealingLR. This prevents the large early gradients from the randomly-initialized head from destroying pretrained ImageNet features.

**Label smoothing + class weights.** FER2013 is severely imbalanced (`disgust`: 436 samples vs `happy`: 7,215). Hard one-hot labels combined with this imbalance cause the model to ignore minority classes. Label smoothing (Îµ=0.1) distributes probability mass across non-target classes, and per-class weights inversely proportional to class frequency are folded into the loss.

**Uncertainty visualization.** The model is a closed-set classifier , it always outputs a distribution over 7 classes even when the input is ambiguous. When `max(softmax) < 0.40`, the top-2 emotions are shown together (e.g. `NEUTRAL / ANGRY  34% / 28%`) and the bounding box turns muted yellow. This is a direct response to the known neutral/angry confusion in FER2013 and reflects the literature on closed-set classifier limitations.

---

## ğŸ“ Project Structure

```
Live-FER/
â”‚
â”œâ”€â”€ configs/
â”‚   â””â”€â”€ config.yaml              # Single source of truth for all hyperparameters
â”‚
â”œâ”€â”€ data/
â”‚   â””â”€â”€ fer2013/                 # Dataset root (not tracked by git)
â”‚       â”œâ”€â”€ train/
â”‚       â”‚   â”œâ”€â”€ angry/           # ~3,995 images
â”‚       â”‚   â”œâ”€â”€ disgust/         # ~436 images  â† heavily underrepresented
â”‚       â”‚   â”œâ”€â”€ fear/            # ~4,097 images
â”‚       â”‚   â”œâ”€â”€ happy/           # ~7,215 images â† dominant class
â”‚       â”‚   â”œâ”€â”€ neutral/         # ~4,965 images
â”‚       â”‚   â”œâ”€â”€ sad/             # ~4,830 images
â”‚       â”‚   â””â”€â”€ surprise/        # ~3,171 images
â”‚       â””â”€â”€ test/
â”‚           â””â”€â”€ (same structure)
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â”œâ”€â”€ dataset.py           # FER2013Dataset + automatic class weight computation
â”‚   â”‚   â””â”€â”€ transforms.py        # Train / val / inference transform pipelines
â”‚   â”‚
â”‚   â”œâ”€â”€ model/
â”‚   â”‚   â”œâ”€â”€ resnet_fer.py        # ResNet-18 adapted for 1-ch, 48Ã—48 FER
â”‚   â”‚   â””â”€â”€ cam.py               # Grad-CAM extractor + heatmap overlay
â”‚   â”‚
â”‚   â”œâ”€â”€ training/
â”‚   â”‚   â”œâ”€â”€ trainer.py           # Training loop, TensorBoard, early stopping
â”‚   â”‚   â””â”€â”€ losses.py            # WeightedCrossEntropy + LabelSmoothingLoss
â”‚   â”‚
â”‚   â”œâ”€â”€ inference/
â”‚   â”‚   â”œâ”€â”€ predictor.py         # Single-image predictor + sliding-window smoother
â”‚   â”‚   â””â”€â”€ webcam.py            # Live webcam loop with Haar Cascade + overlay
â”‚   â”‚
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ metrics.py           # AverageMeter, accuracy(), evaluate_model()
â”‚       â””â”€â”€ visualization.py     # Confusion matrix plot, per-class bar overlay
â”‚
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ train.py                 # Training entry point
â”‚   â”œâ”€â”€ evaluate.py              # Test-set evaluation + confusion matrix export
â”‚   â””â”€â”€ run_webcam.py            # Local live inference entry point
â”‚
â”œâ”€â”€ webapp/
â”‚   â”œâ”€â”€ backend/
â”‚   â”‚   â”œâ”€â”€ app.py               # FastAPI WebSocket server (Hugging Face Spaces)
â”‚   â”‚   â””â”€â”€ main.py              # FastAPI WebSocket server (Railway / local)
â”‚   â””â”€â”€ frontend/
â”‚       â”œâ”€â”€ src/
â”‚       â”‚   â”œâ”€â”€ App.js           # React app , webcam capture, WebSocket client, overlay
â”‚       â”‚   â””â”€â”€ index.js
â”‚       â”œâ”€â”€ .env                 # ws://localhost:8000/ws
â”‚       â”œâ”€â”€ .env.production      # wss://para0107-live-fer-backend.hf.space/ws
â”‚       â”œâ”€â”€ package.json
â”‚       â””â”€â”€ vercel.json          # Vercel deployment config
â”‚
â”œâ”€â”€ checkpoints/                 # Saved .pth files (not tracked)
â”œâ”€â”€ logs/                        # TensorBoard runs + exported figures
â”œâ”€â”€ Dockerfile                   # For Hugging Face Spaces (port 7860)
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## âš™ï¸ Setup

### 1 , Environment

```bash
git clone https://github.com/para0107/Live-Facial-Emotion-Recognition
cd Live-FER

python -m venv .venv
source .venv/bin/activate        # Windows: .venv\Scripts\activate

pip install --upgrade pip
pip install -r requirements.txt
```

### 2 , GPU check (recommended)

```python
import torch
print("CUDA available:", torch.cuda.is_available())
print("Device:", torch.cuda.get_device_name(0) if torch.cuda.is_available() else "CPU")
```

Training on CPU is possible but will take significantly longer (~8â€“12 hours vs ~45 minutes on a mid-range GPU).

---

## ğŸ“¦ Dataset

Download **FER2013** from Kaggle:
ğŸ”— [https://www.kaggle.com/datasets/msambare/fer2013](https://www.kaggle.com/datasets/msambare/fer2013)

Place the extracted `train/` and `test/` folders inside `data/fer2013/`.

### Dataset statistics

| Class     | Train  | Test  | % of Train | Auto Weight |
|-----------|--------|-------|------------|-------------|
| angry     | 3,995  | 958   | 13.9%      | 1.74        |
| disgust   | 436    | 111   | 1.5%       | **15.94**   |
| fear      | 4,097  | 1,024 | 14.3%      | 1.70        |
| happy     | 7,215  | 1,774 | 25.1%      | 0.97        |
| neutral   | 4,965  | 1,233 | 17.3%      | 1.40        |
| sad       | 4,830  | 1,247 | 16.8%      | 1.44        |
| surprise  | 3,171  | 831   | 11.0%      | 2.19        |
| **Total** | **28,709** | **7,178** | , | , |

Class weights are computed automatically in `dataset.py`: `w_c = N_total / (C Ã— N_c)`

---

## ğŸ‹ï¸ Training

```bash
python scripts/train.py
```

All hyperparameters live in `configs/config.yaml`.

### Hyperparameter reference

| Parameter | Value | Rationale |
|-----------|-------|-----------|
| Backbone | ResNet-18 | Good capacity/size ratio for 48Ã—48 images |
| Pretrained | ImageNet | Exploits general visual feature hierarchy |
| Input channels | 1 | FER2013 is grayscale |
| Input size | 48 Ã— 48 | Native FER2013 resolution |
| Freeze epochs | 5 | Head stabilizes before backbone unfreezes |
| Epochs | 50 | With early stopping patience=10 |
| Batch size | 64 | Stable gradients, fits ~4 GB VRAM |
| Learning rate | 2e-4 | Conservative; preserves pretrained weights |
| Weight decay | 1e-4 | L2 regularization |
| Optimizer | Adam | Adaptive LR, robust to sparse gradients |
| Scheduler | CosineAnnealingLR | Smooth decay to eta_min=1e-6 |
| Dropout | 0.5 / 0.25 | Two-stage in classifier head |
| Label smoothing | 0.1 | Prevents overconfident predictions |
| Random erasing | p=0.3 | Forces holistic facial feature use |
| Early stopping | patience=10 | |
| Uncertainty threshold | 0.40 | Below this â†’ show top-2 emotions |

### Monitor with TensorBoard

```bash
tensorboard --logdir logs/
# Open http://localhost:6006
```

### Staged training

```
Epochs 1â€“5:   Backbone FROZEN  â†’  head-only training
              Large gradients from random init stay contained

Epoch 6+:     Full network UNFROZEN
              Fine-tunes with low LR decaying via cosine schedule
```

---

## ğŸ“Š Evaluation

```bash
python scripts/evaluate.py --checkpoint checkpoints/best_model.pth
```

Outputs per-class precision, recall, F1 and saves `logs/confusion_matrix.png`.

**Most common confusions on FER2013:**
- `fear` â†” `sad` (both involve downturned features)
- `disgust` â†” `angry` (both involve brow lowering)
- `neutral` â†” `angry` (subtle activation; motivates the uncertainty display)
- `surprise` â†” `fear` (both involve widened eyes)

These reflect genuine perceptual ambiguity , human accuracy on FER2013 is estimated at ~65%.

---

## ğŸ¥ Real-Time Inference (Local Webcam)

```bash
python scripts/run_webcam.py --checkpoint checkpoints/best_model.pth
```

### Controls

| Key | Action |
|-----|--------|
| `Q` | Quit |
| `R` | Reset temporal smoothing buffer |

### Display elements

- Bounding box colored by dominant emotion (muted yellow when uncertain)
- Label + confidence above the box; dual label when `confidence < 0.40`
- 7-class probability bar chart positioned beside the face box
- Face count bottom-left

### Temporal smoothing

Raw per-frame softmax vectors are averaged over a sliding window:

```
pÌ„_t = (1/N) Î£_{i=0}^{N-1} p_{t-i}     N = 10
```

Eliminates jitter from micro-expressions and brief detection instabilities without learnable parameters. At 30 fps this introduces ~333 ms of latency.

---

## ğŸŒ Web App

A full-stack web application is included in `webapp/`, allowing browser-based real-time inference over a WebSocket connection.

### Stack

| Layer | Technology |
|-------|------------|
| Frontend | React 18, Canvas API |
| Backend | FastAPI, WebSocket |
| Deployment (backend) | Hugging Face Spaces (port 7860) |
| Deployment (frontend) | Vercel |

### How it works

1. The React frontend captures webcam frames at 640Ã—480 via `getUserMedia`
2. Frames are JPEG-encoded (quality 0.6) and sent over WebSocket as base64
3. A **ping-pong lock** prevents frame flooding: the next frame is only sent after the server has replied, naturally adapting to server load
4. The FastAPI backend runs Haar Cascade detection and `EmotionPredictor.predict_smoothed()` on each frame
5. Results (bounding boxes, per-class probabilities, uncertainty flag) are returned as JSON
6. The frontend renders bounding boxes and probability bars on a `<canvas>` overlay

### Running locally

**Backend:**
```bash
cd webapp/backend
uvicorn main:app --host 0.0.0.0 --port 8000
```

**Frontend:**
```bash
cd webapp/frontend
npm install
npm start
# Connects to ws://localhost:8000/ws by default
```

### Uncertainty in the web app

The backend applies the same `uncertainty_threshold = 0.40` as the local webcam script. When `top_conf < 0.40`, the response includes both `emotion` + `secondary_emotion` and sets `is_uncertain: true`. The frontend renders a muted yellow box and a split label such as `NEUTRAL/ANGRY 34%/28%`.

---

## ğŸ”¬ Grad-CAM Visualization

```python
from src.model.cam import CAMExtractor
from src.model.resnet_fer import ResNetFER

model = ResNetFER(pretrained=False)
# load checkpoint...

extractor = CAMExtractor(model)
cam, predicted_class = extractor.generate_cam(input_tensor, target_class=3)  # 3 = happy
overlay = extractor.overlay_cam(original_image, cam)
```

The hook is registered on `model.features.layer4`. Expected behavior: the model attends to mouth/cheeks for `happy`, brow region for `angry`, eye region for `fear`/`surprise`. Diffuse or non-facial attention maps indicate the model is learning dataset artifacts.

---

## ğŸ“ˆ Results

| Configuration | Test Accuracy | Notes |
|---------------|---------------|-------|
| Random baseline | 14.3% | Uniform over 7 classes |
| ResNet-18, no pretrain, scratch | ~52% | Overfits quickly |
| ResNet-18, pretrained, full fine-tune | ~65% | Near human-level |
| + Label smoothing Îµ=0.1 | ~67% | Better calibration |
| + Class-weighted loss | ~67â€“68% | Disgust F1 improves significantly |
| + Random erasing p=0.3 | ~68% | Holistic features |
| **+ Staged freeze/unfreeze (final)** | **68.95%** | **Saved checkpoint** |

*Human accuracy on FER2013 â‰ˆ 65%. Results vary across random seeds.*

---

## ğŸ“š References

1. **Zhang et al. (2024).** *Open-Set Facial Expression Recognition.* AAAI 2024. `arXiv:2401.12507`
2. **Schroff et al. (2015).** *FaceNet: A Unified Embedding for Face Recognition and Clustering.* CVPR 2015. `arXiv:1503.03832`
3. **Dewi et al. (2024).** *Real-Time Facial Expression Recognition: Advances, Challenges, and Future Directions.* Vietnam Journal of Computer Science.
4. **He et al. (2016).** *Deep Residual Learning for Image Recognition.* CVPR 2016.
5. **Goodfellow et al. (2013).** *Challenges in Representation Learning* (FER2013 dataset).
6. **Selvaraju et al. (2017).** *Grad-CAM: Visual Explanations from Deep Networks.* ICCV 2017.
7. **Viola & Jones (2001).** *Rapid Object Detection using a Boosted Cascade of Simple Features.* CVPR 2001.

---

<div align="center">
Built as a Bachelor's thesis project in Computer Science.<br/>
Grounded in peer-reviewed FER literature Â· Every design decision justified.
</div>
